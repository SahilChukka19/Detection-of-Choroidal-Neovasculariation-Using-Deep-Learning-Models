{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26057,"status":"ok","timestamp":1696507911057,"user":{"displayName":"Vardhanika Jagtap","userId":"18266382656015690825"},"user_tz":-330},"id":"UmjvHB8bTa-l","outputId":"05450003-5412-4f0d-ad94-919fb8c589ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"UmjvHB8bTa-l"},{"cell_type":"code","execution_count":null,"metadata":{"id":"mCiNxwEwUPRd"},"outputs":[],"source":["!cp /content/drive/MyDrive/vit/helper_functions.py /content"],"id":"mCiNxwEwUPRd"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11148,"status":"ok","timestamp":1696507923820,"user":{"displayName":"Vardhanika Jagtap","userId":"18266382656015690825"},"user_tz":-330},"id":"5NjjBKazYXvE","outputId":"ac6710aa-cf5c-454a-8442-8e43a2ce2bcf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.8.0\n"]}],"source":["!pip install torchinfo"],"id":"5NjjBKazYXvE"},{"cell_type":"code","execution_count":null,"metadata":{"id":"jBMv2GoIXot5"},"outputs":[],"source":["!cp /content/drive/MyDrive/vit/going_modular/engine.py /content\n","!cp /content/drive/MyDrive/vit/going_modular/model_builder.py /content\n","!cp /content/drive/MyDrive/vit/going_modular/predictions.py /content\n","!cp /content/drive/MyDrive/vit/going_modular/train.py /content\n","!cp /content/drive/MyDrive/vit/going_modular/utils.py /content"],"id":"jBMv2GoIXot5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1fb1c10e"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","\n","from torch import nn\n","from torchvision import transforms\n","from helper_functions import set_seeds"],"id":"1fb1c10e"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1696507932215,"user":{"displayName":"Vardhanika Jagtap","userId":"18266382656015690825"},"user_tz":-330},"id":"12e97c71","outputId":"6bc6db9d-9d7a-4aaa-b45b-8f489dc86340"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'cuda'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"],"id":"12e97c71"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15972,"status":"ok","timestamp":1696507948179,"user":{"displayName":"Vardhanika Jagtap","userId":"18266382656015690825"},"user_tz":-330},"id":"0de25b1a","outputId":"8c471197-1290-4377-ec3b-2026737e74a6"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n","100%|██████████| 330M/330M [00:05<00:00, 68.9MB/s]\n"]}],"source":["# 1. Get pretrained weights for ViT-Base\n","pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n","\n","# 2. Setup a ViT model instance with pretrained weights\n","pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n","\n","# 3. Freeze the base parameters\n","for parameter in pretrained_vit.parameters():\n","    parameter.requires_grad = False\n","\n","# 4. Change the classifier head\n","class_names = ['cnv','normal']\n","\n","set_seeds()\n","pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n","# pretrained_vit # uncomment for model output"],"id":"0de25b1a"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9715,"status":"ok","timestamp":1696507957881,"user":{"displayName":"Vardhanika Jagtap","userId":"18266382656015690825"},"user_tz":-330},"id":"e3feaa42","outputId":"cd7b3fbe-b89a-4f29-cfa8-f2eb8a4c669e"},"outputs":[{"data":{"text/plain":["============================================================================================================================================\n","Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n","============================================================================================================================================\n","VisionTransformer (VisionTransformer)                        [32, 3, 224, 224]    [32, 2]              768                  Partial\n","├─Conv2d (conv_proj)                                         [32, 3, 224, 224]    [32, 768, 14, 14]    (590,592)            False\n","├─Encoder (encoder)                                          [32, 197, 768]       [32, 197, 768]       151,296              False\n","│    └─Dropout (dropout)                                     [32, 197, 768]       [32, 197, 768]       --                   --\n","│    └─Sequential (layers)                                   [32, 197, 768]       [32, 197, 768]       --                   False\n","│    │    └─EncoderBlock (encoder_layer_0)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_1)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_2)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_3)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_4)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_5)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_6)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_7)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_8)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_9)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_10)                  [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_11)                  [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    └─LayerNorm (ln)                                        [32, 197, 768]       [32, 197, 768]       (1,536)              False\n","├─Linear (heads)                                             [32, 768]            [32, 2]              1,538                True\n","============================================================================================================================================\n","Total params: 85,800,194\n","Trainable params: 1,538\n","Non-trainable params: 85,798,656\n","Total mult-adds (G): 5.52\n","============================================================================================================================================\n","Input size (MB): 19.27\n","Forward/backward pass size (MB): 3330.74\n","Params size (MB): 229.20\n","Estimated Total Size (MB): 3579.20\n","============================================================================================================================================"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["from torchinfo import summary\n","\n","# Print a summary using torchinfo (uncomment for actual output)\n","summary(model=pretrained_vit,\n","        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n","        # col_names=[\"input_size\"], # uncomment for smaller output\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n",")"],"id":"e3feaa42"},{"cell_type":"markdown","metadata":{"id":"c73ec300"},"source":["#### Notice how only the output layer is trainable, where as, all of the rest of the layers are untrainable (frozen)."],"id":"c73ec300"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ac8cc699"},"outputs":[],"source":["# Setup directory paths to train and test images\n","train_dir = \"/content/drive/MyDrive/dataset80-20/training\"\n","test_dir = \"/content/drive/MyDrive/dataset80-20/testing\""],"id":"ac8cc699"},{"cell_type":"markdown","metadata":{"id":"91175306"},"source":["Remember, if you're going to use a pretrained model, it's generally important to ensure your own custom data is transformed/formatted in the same way the data the original model was trained on."],"id":"91175306"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1696507957881,"user":{"displayName":"Vardhanika Jagtap","userId":"18266382656015690825"},"user_tz":-330},"id":"05aa777b","outputId":"320f03b6-8350-498b-f2d6-7bc4482aa963"},"outputs":[{"name":"stdout","output_type":"stream","text":["ImageClassification(\n","    crop_size=[224]\n","    resize_size=[256]\n","    mean=[0.485, 0.456, 0.406]\n","    std=[0.229, 0.224, 0.225]\n","    interpolation=InterpolationMode.BILINEAR\n",")\n"]}],"source":["# Get automatic transforms from pretrained ViT weights\n","pretrained_vit_transforms = pretrained_vit_weights.transforms()\n","print(pretrained_vit_transforms)"],"id":"05aa777b"},{"cell_type":"markdown","metadata":{"id":"088971e6"},"source":["## And now we've got transforms ready, we can turn our images into DataLoaders using the create_dataloaders()"],"id":"088971e6"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5d49225b"},"outputs":[],"source":["import os\n","\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","NUM_WORKERS = os.cpu_count()\n","\n","def create_dataloaders(\n","    train_dir: str,\n","    test_dir: str,\n","    transform: transforms.Compose,\n","    batch_size: int,\n","    num_workers: int=NUM_WORKERS\n","):\n","\n","  # Use ImageFolder to create dataset(s)\n","  train_data = datasets.ImageFolder(train_dir, transform=transform)\n","  test_data = datasets.ImageFolder(test_dir, transform=transform)\n","\n","  # Get class names\n","  class_names = train_data.classes\n","\n","  # Turn images into data loaders\n","  train_dataloader = DataLoader(\n","      train_data,\n","      batch_size=batch_size,\n","      shuffle=True,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","  )\n","  test_dataloader = DataLoader(\n","      test_data,\n","      batch_size=batch_size,\n","      shuffle=False,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","  )\n","\n","  return train_dataloader, test_dataloader, class_names"],"id":"5d49225b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9037c8a5"},"outputs":[],"source":["# Setup dataloaders\n","train_dataloader_pretrained, test_dataloader_pretrained, class_names = create_dataloaders(train_dir=train_dir,\n","                                                                                                     test_dir=test_dir,\n","                                                                                                     transform=pretrained_vit_transforms,\n","                                                                                                     batch_size=32) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)\n"],"id":"9037c8a5"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":158,"referenced_widgets":["6c03cfc4396c4fc28d0ba34efcea3e4c","4a54b7d45e3d447692398ef245b1859e","9b2f0dd7f78544aab27d5e528cf98747","c89bae67e8c34f3b94e0abc050fa1775","e78e8944c733458e9c2bd302e1879890","338bccd3bcec4af0ab95656067041ae9","20586e42194d41228394acc18b2cfad9","5209e328a9424faeb097d897db55ac68","a9801ef5c08c48f3aa254d2d683526ed","b8dfad3bf5224b1881ea73c6562e6de0","78e19ecfe9164754b74e56ed7aedd391"]},"id":"10c5ba74","outputId":"4a091228-2fe3-4e72-8844-01e72f94bdc0"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6c03cfc4396c4fc28d0ba34efcea3e4c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/50 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 1 | train_loss: 0.0794 | train_acc: 0.9772 | test_loss: 0.0760 | test_acc: 0.9715\n","Epoch: 2 | train_loss: 0.0328 | train_acc: 0.9907 | test_loss: 0.0633 | test_acc: 0.9783\n","Epoch: 3 | train_loss: 0.0260 | train_acc: 0.9925 | test_loss: 0.0598 | test_acc: 0.9821\n","Epoch: 4 | train_loss: 0.0228 | train_acc: 0.9932 | test_loss: 0.0592 | test_acc: 0.9827\n","Epoch: 5 | train_loss: 0.0201 | train_acc: 0.9947 | test_loss: 0.0563 | test_acc: 0.9808\n","Epoch: 6 | train_loss: 0.0180 | train_acc: 0.9954 | test_loss: 0.0690 | test_acc: 0.9811\n"]}],"source":["import engine\n","\n","# Create optimizer and loss function\n","optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),\n","                             lr=1e-3)\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","# Train the classifier head of the pretrained ViT feature extractor model\n","set_seeds()\n","pretrained_vit_results = engine.train(model=pretrained_vit,\n","                                      train_dataloader=train_dataloader_pretrained,\n","                                      test_dataloader=test_dataloader_pretrained,\n","                                      optimizer=optimizer,\n","                                      loss_fn=loss_fn,\n","                                      epochs=50,\n","                                      device=device)"],"id":"10c5ba74"},{"cell_type":"code","execution_count":null,"metadata":{"id":"u8nEG7kIo8OQ"},"outputs":[],"source":["import torch\n","import h5py\n","import numpy as np\n","\n","# Assuming you have a trained Vision Transformer model 'pretrained_vit'\n","\n","# 1. Convert the PyTorch model to a dictionary\n","model_state_dict = pretrained_vit.state_dict()\n","\n","# Add other necessary information if needed, e.g., hyperparameters, model architecture details\n","model_info = {\n","    'architecture': 'Vision Transformer',\n","    'input_size': (224, 224),  # Input image size\n","    'num_classes': 2,  # Number of output classes\n","    'other_info': 'Your additional information here',\n","}\n","\n","# 2. Save the dictionary to an HDF5 file\n","h5_filename = \"/content/drive/MyDrive/models 80-20/vit-model-80-20-50ep.h5\"  # Specify the name of the HDF5 file\n","\n","with h5py.File(h5_filename, 'w') as h5_file:\n","    # Save the model state_dict\n","    for key, value in model_state_dict.items():\n","        h5_file.create_dataset(key, data=value.cpu().numpy())  # Convert tensors to numpy arrays\n","\n","    # Save other model information as attributes\n","    for key, value in model_info.items():\n","        h5_file.attrs[key] = str(value)\n","\n","print(f\"Model saved to {h5_filename}\")\n","\n","# 3. Save the model using torch.save\n","checkpoint = {\n","    'model_state_dict': pretrained_vit.state_dict(),\n","    # You can add more information if needed, such as optimizer state, hyperparameters, etc.\n","}\n","\n","# Define the path where you want to save the model\n","save_path = '/content/drive/MyDrive/models 80-20/vit_model-80-20-50ep.pth'\n","\n","# Save the checkpoint to the specified path\n","torch.save(checkpoint, save_path)\n","\n","print(f\"Model saved to {save_path}\")\n"],"id":"u8nEG7kIo8OQ"},{"cell_type":"markdown","metadata":{"id":"945149b4"},"source":["pretrained ViT performed far better than our custom ViT model trained from scratch (in the same amount of time).\n"],"id":"945149b4"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2aae16a8"},"outputs":[],"source":["# Plot the loss curves\n","from helper_functions import plot_loss_curves\n","\n","plot_loss_curves(pretrained_vit_results)"],"id":"2aae16a8"},{"cell_type":"markdown","metadata":{"id":"79922fd1"},"source":["## That's the power of transfer learning!\n","\n","We managed to get outstanding results with the same model architecture, except our custom implementation was trained from scratch (worse performance) and this feature extractor model has the power of pretrained weights from ImageNet behind it."],"id":"79922fd1"},{"cell_type":"markdown","metadata":{"id":"6ce4427b"},"source":["# Let's make Prediction:"],"id":"6ce4427b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"fe89b4e9"},"outputs":[],"source":["import requests\n","\n","# Import function to make predictions on images and plot them\n","from predictions import pred_and_plot_image\n","\n","from PIL import Image\n","import tempfile\n","\n","# Load the grayscale image\n","gray_image = Image.open(\"/content/drive/MyDrive/cnv sample/CNV-743852-1.jpeg\")\n","\n","# Convert the grayscale image to RGB format\n","rgb_image = gray_image.convert(\"RGB\")\n","temp_rgb_path = tempfile.NamedTemporaryFile(delete=False, suffix=\".jpg\")\n","rgb_image.save(temp_rgb_path.name)\n","# Predict on custom image\n","pred_and_plot_image(model=pretrained_vit,\n","                    image_path=temp_rgb_path.name,\n","                    class_names=class_names)"],"id":"fe89b4e9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5e7d64d0"},"outputs":[],"source":["import requests\n","\n","# Import function to make predictions on images and plot them\n","from predictions import pred_and_plot_image\n","\n","from PIL import Image\n","import tempfile\n","\n","# Load the grayscale image\n","gray_image = Image.open(\"/content/drive/MyDrive/normal sample/NORMAL-508852-2.jpeg\")\n","\n","# Convert the grayscale image to RGB format\n","rgb_image = gray_image.convert(\"RGB\")\n","temp_rgb_path = tempfile.NamedTemporaryFile(delete=False, suffix=\".jpg\")\n","rgb_image.save(temp_rgb_path.name)\n","# Predict on custom image\n","pred_and_plot_image(model=pretrained_vit,\n","                    image_path=temp_rgb_path.name,\n","                    class_names=class_names)"],"id":"5e7d64d0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9hxcvnR9tTJi"},"outputs":[],"source":["test1_dir = \"/content/drive/MyDrive/testing\"\n","import os\n","\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","NUM_WORKERS = os.cpu_count()\n","\n","def create_dataloaders(\n","    test1_dir: str\n","):\n","\n","  # Use ImageFolder to create dataset(s)\n","\n","  test1_data = datasets.ImageFolder(test1_dir, transform=transform)\n","\n","  # Get class names\n","  class_names = train_data.classes\n","\n","  test1_dataloader = DataLoader(\n","      test1_data,\n","      batch_size=batch_size,\n","      shuffle=False,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","  )\n","\n","  return test1_dataloader, class_names"],"id":"9hxcvnR9tTJi"},{"cell_type":"code","execution_count":null,"metadata":{"id":"eb7d0f9e"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n","\n","def evaluate_model(model, test1_dataloader, device):\n","    model.eval()\n","    true_labels = []\n","    predicted_labels = []\n","\n","    with torch.no_grad():\n","        for images, labels in test1_dataloader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","\n","            true_labels.extend(labels.cpu().numpy())\n","            predicted_labels.extend(predicted.cpu().numpy())\n","\n","    # Calculate accuracy\n","    accuracy = accuracy_score(true_labels, predicted_labels)\n","    print(\"Accuracy:\", accuracy)\n","\n","    # Generate the confusion matrix\n","    cm = confusion_matrix(true_labels, predicted_labels)\n","    print(\"Confusion Matrix:\\n\", cm)\n","\n","    # Generate the classification report\n","    class_names = test1_dataloader.dataset.classes\n","    report = classification_report(true_labels, predicted_labels, target_names=class_names)\n","    print(\"Classification Report:\\n\", report)\n","\n","# Evaluate the pretrained ViT model\n","evaluate_model(pretrained_vit, test1_dataloader_pretrained, device)\n"],"id":"eb7d0f9e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"yntvVlJPv0W0"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from sklearn.metrics import roc_curve, roc_auc_score, auc\n","import torch\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","\n","# Load your saved model\n","model_path = '/content/drive/MyDrive/models 80-20/vit_model-80-20-50ep.pth'\n","loaded_model = torch.load(model_path)\n","\n","# Set the model to evaluation mode\n","loaded_model.eval()\n","\n","# Load your validation dataset (val_dataset) with labels\n","# You can use tfds.load or tf.keras.utils.image_dataset_from_directory to load the dataset\n","\n","# Define a function to load your validation dataset with labels\n","def load_test_data(test1_dir, transform):\n","    test1_dataset = datasets.ImageFolder(root=test1_dir, transform=transform)\n","    return DataLoader(test1_dataset, batch_size=32, shuffle=False)\n","\n","# Define the data transformation for the validation dataset (similar to what you used for training)\n","test1_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","])\n","\n","# Load your validation dataset\n","test1_dir = \"/content/drive/MyDrive/testing\"\n","test1_loader = load_test_data(test1_dir, test1_transform)\n","\n","# Predict probabilities on the validation dataset\n","y_true = []\n","y_probs = []\n","\n","for images, labels in test1_loader:\n","    images = images.to(device)  # Assuming 'device' is correctly set to GPU\n","    labels = labels.to(device)\n","    outputs = loaded_model(images)\n","    probabilities = torch.softmax(outputs, dim=1)[:, 1]  # Assuming binary classification, get probabilities of class '1'\n","    y_true.extend(labels.cpu().numpy())\n","    y_probs.extend(probabilities.cpu().detach().numpy())\n","\n","# Convert to numpy arrays\n","y_true = np.array(y_true)\n","y_probs = np.array(y_probs)\n","\n","# Calculate ROC curve and AUC for each class\n","fpr = {}\n","tpr = {}\n","roc_auc = {}\n","\n","class_names = ['cnv', 'normal']\n","\n","for i in range(len(class_names)):\n","    fpr[i], tpr[i], _ = roc_curve(y_true == i, y_probs)\n","    roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","# Plot ROC curves\n","plt.figure(figsize=(10, 6))\n","for i in range(len(class_names)):\n","    plt.plot(fpr[i], tpr[i], label=f'ROC curve ({class_names[i]} AUC = {roc_auc[i]:.2f})')\n","\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curves for Classifiers')\n","plt.legend(loc=\"lower right\")\n","plt.show()\n"],"id":"yntvVlJPv0W0"}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"20586e42194d41228394acc18b2cfad9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"338bccd3bcec4af0ab95656067041ae9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a54b7d45e3d447692398ef245b1859e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_338bccd3bcec4af0ab95656067041ae9","placeholder":"​","style":"IPY_MODEL_20586e42194d41228394acc18b2cfad9","value":" 12%"}},"5209e328a9424faeb097d897db55ac68":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c03cfc4396c4fc28d0ba34efcea3e4c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4a54b7d45e3d447692398ef245b1859e","IPY_MODEL_9b2f0dd7f78544aab27d5e528cf98747","IPY_MODEL_c89bae67e8c34f3b94e0abc050fa1775"],"layout":"IPY_MODEL_e78e8944c733458e9c2bd302e1879890"}},"78e19ecfe9164754b74e56ed7aedd391":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b2f0dd7f78544aab27d5e528cf98747":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_5209e328a9424faeb097d897db55ac68","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a9801ef5c08c48f3aa254d2d683526ed","value":6}},"a9801ef5c08c48f3aa254d2d683526ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b8dfad3bf5224b1881ea73c6562e6de0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c89bae67e8c34f3b94e0abc050fa1775":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8dfad3bf5224b1881ea73c6562e6de0","placeholder":"​","style":"IPY_MODEL_78e19ecfe9164754b74e56ed7aedd391","value":" 6/50 [1:15:04&lt;4:40:17, 382.22s/it]"}},"e78e8944c733458e9c2bd302e1879890":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":5}